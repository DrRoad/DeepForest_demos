{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study - Drones\n",
    "\n",
    "This is a demonstration notebook of using DeepForest to predict and train models for individual tree segmentation in drone imagery. The goal of this notebook is to orient users to the general DeepForest workflow. Due to data sharing agreements, the data in this example is not available. For more information on the data see the excellent: \n",
    "\n",
    "Aubry-Kientz, M., Dutrieux, R., Ferraz, A., Saatchi, S., Hamraz, H., Williams, J., Coomes, D., Piboule, A., Vincent, G., 2019. A Comparative Assessment of the Performance of Individual Tree Crowns Delineation Algorithms from ALS Data in Tropical Forests. Remote Sens. 11, 1086. https://doi.org/10.3390/rs11091086\n",
    "\n",
    "I would like to thank the authors for sharing their data to test deepforest performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load base packages\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Load deepforest\n",
    "#Optional comet_ml for tracking experiments\n",
    "from comet_ml import Experiment\n",
    "from deepforest import deepforest\n",
    "from deepforest import preprocess\n",
    "from deepforest import utilities\n",
    "from deepforest import __version__\n",
    "\n",
    "#Geospatial packages\n",
    "import shapely\n",
    "import geopandas\n",
    "import rasterio\n",
    "\n",
    "#Check version\n",
    "print(\"This demo is run with deepforest version {}\".format(__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are version warnings for tensorflow and numpy and can be ignored. Depending on your operating system and specific build, there may be more deprecation warnings. I am happy to field any questions on the DeepForest git repo issues page: https://github.com/weecology/DeepForest/issues  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with a new dataset, the first step is usually test the performance of the prebuilt model. For information on how this model was created see https://deepforest.readthedocs.io/en/latest/getting_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DeepForest prebuilt model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new deepforest object and download the latest release of the prebuilt NEON model from github: https://github.com/weecology/DeepForest/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deepforest.deepforest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These warnings are expected. We have decided not to suppress them in case they conflict with users processes in other codebases. As for DeepForest, they can be safely ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__release_version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RGB Raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to look at small piece of a large tile before performing predict_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = \"/Users/ben/Dropbox/Weecology/Drone/ForBen/RGB_allPlots/RetroProj_cropped/RP_2015_P15.tif\"\n",
    "raster = Image.open(raster_path)\n",
    "numpy_image = np.array(raster)\n",
    "numpy_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our image is 3440 by 3440 pixels with 3 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop a small window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = preprocess.compute_windows(numpy_image, patch_size=800,patch_overlap=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "windows is a sliding window object that holds the position of each crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 9\n",
    "crop = numpy_image[windows[index].indices()] \n",
    "crop.shape\n",
    "plt.imshow(crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change from RGB to BGR channel order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib and OpenCV differ in default channel orders. Using Image.open will yield RGB images, whereas keras-retinanet expects BGR images. Flip channel order using numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = crop[...,::-1] #keras-retinanet requires bluegreered channel order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict from the prebuilt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_image(raw_image = crop,return_plot=True, score_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(prediction[...,::-1]) #show in rgb channel order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the bounding boxes as a dataframe, use return_plot=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_boxes = model.predict_image(raw_image = crop,return_plot=False, score_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_boxes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, = plt.subplots(nrows=4,ncols=3, figsize=(30,30))\n",
    "axes = axes.flatten()\n",
    "for index in range(12):\n",
    "    crop = numpy_image[windows[index].indices()] \n",
    "    #predict in bgr channel order\n",
    "    prediction = model.predict_image(raw_image = crop[...,::-1],return_plot=True, score_threshold=0.05)\n",
    "    #but plot in rgb channel order\n",
    "    axes[index].imshow(prediction[...,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict entire tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall tile is too large to fit into memory. DeepForest will slide overlapping windows, same as the sized above, make a prediction on each window. Then reassemble and delete overlaps based on the highest scoring box. The reassembling process can be subtle, and requires the user to balance the amount of overlap (more predictions = slower), and the overall size of objects on interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = model.predict_tile(raster_path,return_plot=True,patch_size=800,patch_overlap=0.15,iou_threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "plt.imshow(tile)\n",
    "plt.savefig(\"/Users/Ben/Desktop/overlap30_iou20.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create shapefile of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular use case, we are interested in taking the bounding boxes and making a shapefile of bounding boxes. This section requires dependencies outside of DeepForest, and in particular geopandas can be annoying to install on windows due to GDAL. For this reason, these packages are not included in the main DeepForest install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = model.predict_tile(raster_path,return_plot=False,patch_size=800,patch_overlap=0.15,iou_threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Georeference.\n",
    "#This isn't a true projection, but over small spatial scales it will be fine. Add the origin of the raster and multiply the box height and width by the cell size (m/cell).\n",
    "#read in extent using rasterio\n",
    "with rasterio.open(raster_path) as dataset:\n",
    "    bounds = dataset.bounds\n",
    "    pixelSizeX, pixelSizeY  = dataset.res\n",
    "\n",
    "#subtract origin. Recall that numpy origin is top left! Not bottom left.\n",
    "boxes[\"xmin\"] = (boxes[\"xmin\"] *pixelSizeX) + bounds.left\n",
    "boxes[\"xmax\"] = (boxes[\"xmax\"] * pixelSizeX) + bounds.left\n",
    "boxes[\"ymin\"] = bounds.top - (boxes[\"ymin\"] * pixelSizeY) \n",
    "boxes[\"ymax\"] = bounds.top - (boxes[\"ymax\"] * pixelSizeY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine column to a shapely Box() object, save shapefile\n",
    "boxes['geometry'] = boxes.apply(lambda x: shapely.geometry.box(x.xmin,x.ymin,x.xmax,x.ymax), axis=1)\n",
    "boxes = geopandas.GeoDataFrame(boxes, geometry='geometry')\n",
    "\n",
    "#set projection, (see dataset.crs) hard coded here\n",
    "boxes.crs = {'init' :'epsg:32622'}\n",
    "#get proj info see:https://gis.stackexchange.com/questions/204201/geopandas-to-file-saves-geodataframe-without-coordinate-system\n",
    "prj = 'PROJCS[\"WGS_1984_UTM_Zone_22N\",GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-51],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1]]'\n",
    "boxes.to_file('PrebuiltModel.shp', driver='ESRI Shapefile',crs_wkt=prj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning from the prebuilt model test above, we can train a model based on annotations from the study site in question. A seperate tile was hand-annotated using the program rectlabel. See https://deepforest.readthedocs.io/en/latest/training.html. There is no fixed number of annotations needed, it depends on the variability of the target dataset. Our experience is that ~1000 crowns is a good starting point. See Figure 8 from https://www.biorxiv.org/content/10.1101/790071v1.full. This example took about 4 hours to annotate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training Annotations](figures/TrainingData.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training configurations are stored in a deepforest_config.yml in the local directory, or a default config is used from install. These parameters can be accessed at runtime as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the parameter settings see https://deepforest.readthedocs.io/en/latest/training_config.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare annotation data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepForest trains on 400x400 pixel windows. The overall image is too large to fit into memory. We therefore need to split the annotations into windows and generate the crops for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read xml and create pandas frame\n",
    "annotation_paths = glob.glob(\"/Users/ben/Dropbox/Weecology/Drone/*.xml\")\n",
    "print(annotation_paths)\n",
    "annotation_list = []\n",
    "for xml in annotation_paths:\n",
    "    xml_parse = utilities.xml_to_annotations(xml)\n",
    "\n",
    "    #Write converted dataframe to file. Saved alongside the images. This is a temporary file for cutting windows.\n",
    "    xml_parse.to_csv(\"FG_example.csv\", index=False)\n",
    "\n",
    "    #get plot name to match to RGB image\n",
    "    plot_name = re.search(\"2015_(\\w+).xml\",xml).group(1)\n",
    "    \n",
    "    #Where to save cropped images\n",
    "    crop_dir = \"crops/\"\n",
    "    result = preprocess.split_raster(path_to_raster=\"/Users/ben/Dropbox/Weecology/Drone/ForBen/RGB_allPlots/RetroProj_cropped/RP_2015_{}.tif\".format(plot_name),\n",
    "                                     annotations_file=\"FG_example.csv\",\n",
    "                                     base_dir=crop_dir,\n",
    "                                     patch_size=800,\n",
    "                                     patch_overlap=0.05)\n",
    "    annotation_list.append(result)\n",
    "\n",
    "train_annotations = pd.concat(annotation_list)\n",
    "\n",
    "#Split image crops into training and test. Normally these would be different tiles! Just as an example.\n",
    "image_paths = train_annotations.image_path.unique()\n",
    "test_paths = np.random.choice(image_paths, 3)\n",
    "test_annotations = train_annotations.loc[train_annotations.image_path.isin(test_paths)]\n",
    "train_annotations = train_annotations.loc[~train_annotations.image_path.isin(test_paths)]\n",
    "\n",
    "#View output\n",
    "train_annotations.head()\n",
    "print(\"There are {} training crown annotations\".format(train_annotations.shape[0]))\n",
    "print(\"There are {} test crown annotations\".format(test_annotations.shape[0]))\n",
    "\n",
    "#save to file\n",
    "#Write window annotations file without a header row, same location as the \"base_dir\" above.\n",
    "train_annotations.to_csv(crop_dir + \"train.csv\",index=False,header=False)\n",
    "test_annotations.to_csv(crop_dir + \"test.csv\",index=False,header=False)\n",
    "\n",
    "annotations_file= crop_dir + \"train.csv\"\n",
    "test_file= crop_dir + \"test.csv\"\n",
    "\n",
    "#Add the test annotations to the config, see https://deepforest.readthedocs.io/en/latest/training_config.html#validation-annotations-none\n",
    "model.config[\"validation_annotations\"] = test_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training lets get a bit of a baseline by getting the evaluation score on the test file using the prebuilt model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally create a comet experiment. Comet is a machine learning visualization dashboard with great tools and project tracking. There is a free tier for academics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_experiment = Experiment(api_key=\"ypQZhYfs3nSyKzOfz13iuJpj2\",\n",
    "                                  project_name=\"frenchguiana\", workspace=\"bw4sz\")\n",
    "\n",
    "#Train for a bit longer\n",
    "model.config[\"epochs\"] = 5\n",
    "comet_experiment.log_parameters(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(annotations=annotations_file,input_type=\"fit_generator\",comet_experiment=comet_experiment)\n",
    "comet_experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accurary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to predict the annotations used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(annotations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we held out a few crops from training. In general, we recommend that any validation data be from a seperate geographic tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A jump from 0.39 to 0.55 in ten minutes of training on a laptop. That's an improvement of 63% on out of sample data. Some caveats needed for the spatial autocorrelation in training and test data. We always recommend having geographically seperate data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict new tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_tile = model.predict_tile(raster_path,return_plot=True,patch_size=800,patch_overlap=0.15,iou_threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "plt.imshow(trained_model_tile)\n",
    "plt.savefig(\"/Users/Ben/Desktop/overlap30_iou20_trained.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_boxes = model.predict_tile(raster_path,return_plot=False,patch_size=800,patch_overlap=0.15,iou_threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add raster extent to boxes to place in utm system. \n",
    "#This isn't a true projection, but over small spatial scales will be fine. Add the origin of the raster and multiply the box height and width by the cell size (m/cell).\n",
    "#read in extent using rasterio\n",
    "with rasterio.open(raster_path) as dataset:\n",
    "    bounds = dataset.bounds\n",
    "    pixelSizeX, pixelSizeY  = dataset.res\n",
    "\n",
    "#subtract origin. Recall that numpy origin is top left! Not bottom left.\n",
    "trained_model_boxes[\"xmin\"] = (trained_model_boxes[\"xmin\"] * pixelSizeX) + bounds.left\n",
    "trained_model_boxes[\"xmax\"] = (trained_model_boxes[\"xmax\"] * pixelSizeX) + bounds.left\n",
    "trained_model_boxes[\"ymin\"] = bounds.top - (trained_model_boxes[\"ymin\"] * pixelSizeY) \n",
    "trained_model_boxes[\"ymax\"] = bounds.top - (trained_model_boxes[\"ymax\"] * pixelSizeY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine lat and lon column to a shapely Box() object, save shapefile\n",
    "trained_model_boxes['geometry'] = trained_model_boxes.apply(lambda x: shapely.geometry.box(x.xmin,x.ymin,x.xmax,x.ymax), axis=1)\n",
    "trained_model_boxes = geopandas.GeoDataFrame(trained_model_boxes, geometry='geometry')\n",
    "trained_model_boxes.crs = {'init' :'epsg:32622'}\n",
    "#manually look up epsg\n",
    "prj = 'PROJCS[\"WGS_1984_UTM_Zone_22N\",GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-51],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1]]'\n",
    "trained_model_boxes.to_file('TrainedModel.shp', driver='ESRI Shapefile',crs_wkt=prj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking advantage of the fact that numpy slicing operates on the initial dataframe, we can specify the same crops as above and plot against the prebuilt model predictions boxes in a new color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, = plt.subplots(nrows=2,ncols=3, figsize=(30,30))\n",
    "axes = axes.flatten()\n",
    "for index in range(6):\n",
    "    crop = numpy_image[windows[index].indices()] \n",
    "    #predict in bgr channel order\n",
    "    prediction = model.predict_image(raw_image = crop[...,::-1],return_plot=True, score_threshold=0.05,color=(255,0,0))\n",
    "    #but plot in rgb channel order\n",
    "    axes[index].imshow(prediction[...,::-1])\n",
    "    axes[index].get_yaxis().set_visible(False)\n",
    "    axes[index].get_xaxis().set_visible(False)\n",
    "fig.tight_layout() \n",
    "plt.show()\n",
    "fig.savefig(\"predictions.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local annotations only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to know whether the local annotations, with no pretraining weights from the prebuilt model, perform as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = deepforest.deepforest()\n",
    "comet_experiment = Experiment(api_key=\"ypQZhYfs3nSyKzOfz13iuJpj2\",\n",
    "                                  project_name=\"frenchguiana\", workspace=\"bw4sz\")\n",
    "\n",
    "#Train for a bit longer\n",
    "local_model.config[\"epochs\"] = 5\n",
    "model.config[\"validation_annotations\"] = test_file\n",
    "comet_experiment.log_parameters(local_model.config)\n",
    "local_model.train(annotations=annotations_file,input_type=\"fit_generator\",comet_experiment=comet_experiment)\n",
    "comet_experiment.end()\n",
    "local_model.evaluate_generator(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_boxes = local_model.predict_tile(raster_path,return_plot=False,patch_size=800,patch_overlap=0.15,iou_threshold=0.15)\n",
    "# Add raster extent to boxes to place in utm system. \n",
    "#This isn't a true projection, but over small spatial scales will be fine. Add the origin of the raster and multiply the box height and width by the cell size (m/cell).\n",
    "#read in extent using rasterio\n",
    "\n",
    "with rasterio.open(raster_path) as dataset:\n",
    "    bounds = dataset.bounds\n",
    "    pixelSizeX, pixelSizeY  = dataset.res\n",
    "\n",
    "#subtract origin. Recall that numpy origin is top left! Not bottom left.\n",
    "trained_model_boxes[\"xmin\"] = (trained_model_boxes[\"xmin\"] * pixelSizeX) + bounds.left\n",
    "trained_model_boxes[\"xmax\"] = (trained_model_boxes[\"xmax\"] * pixelSizeX) + bounds.left\n",
    "trained_model_boxes[\"ymin\"] = bounds.top - (trained_model_boxes[\"ymin\"] * pixelSizeY) \n",
    "trained_model_boxes[\"ymax\"] = bounds.top - (trained_model_boxes[\"ymax\"] * pixelSizeY)\n",
    "\n",
    "# combine lat and lon column to a shapely Box() object, save shapefile\n",
    "trained_model_boxes['geometry'] = trained_model_boxes.apply(lambda x: shapely.geometry.box(x.xmin,x.ymin,x.xmax,x.ymax), axis=1)\n",
    "trained_model_boxes = geopandas.GeoDataFrame(trained_model_boxes, geometry='geometry')\n",
    "trained_model_boxes.crs = {'init' :'epsg:32622'}\n",
    "#manually look up epsg\n",
    "prj = 'PROJCS[\"WGS_1984_UTM_Zone_22N\",GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-51],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1]]'\n",
    "trained_model_boxes.to_file('LocalModel.shp', driver='ESRI Shapefile',crs_wkt=prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
